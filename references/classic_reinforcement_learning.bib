@article{bellman1966dynamic,
    title={Dynamic programming},
    author={Bellman, Richard},
    journal={science},
    volume={153},
    number={3731},
    pages={34--37},
    year={1966},
    publisher={American Association for the Advancement of Science},
    note={\href{https://gwern.net/doc/statistics/decision/1957-bellman-dynamicprogramming.pdf}{\textbf{Book}}},
}

@article{puterman1978modified,
    title={Modified policy iteration algorithms for discounted Markov decision problems},
    author={Puterman, Martin L and Shin, Moon Chirl},
    journal={Management Science},
    volume={24},
    number={11},
    pages={1127--1137},
    year={1978},
    publisher={INFORMS},
    note={\href{https://pubsonline.informs.org/doi/abs/10.1287/mnsc.24.11.1127}{\textbf{Article}}},
}



@article{sutton1988learning,
    title={Learning to predict by the methods of temporal differences},
    author={Sutton, Richard S},
    journal={Machine learning},
    volume={3},
    number={1},
    pages={9--44},
    year={1988},
    publisher={Springer},
    note={\href{https://link.springer.com/article/10.1007/bf00115009}{\textbf{Article}}},
}


@article{watkins1992q,
    title={Q-learning},
    author={Watkins, Christopher JCH and Dayan, Peter},
    journal={Machine learning},
    volume={8},
    number={3},
    pages={279--292},
    year={1992},
    publisher={Springer},
    note={\href{https://link.springer.com/article/10.1007/bf00992698}{\textbf{Article}}},
}

@article{williams1992simple,
    title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    author={Williams, Ronald J},
    journal={Machine learning},
    volume={8},
    number={3},
    pages={229--256},
    year={1992},
    publisher={Springer},
}


@book{rummery1994line,
    title={On-line Q-learning using connectionist systems},
    author={Rummery, Gavin A and Niranjan, Mahesan},
    volume={37},
    year={1994},
    publisher={University of Cambridge, Department of Engineering Cambridge, UK},
    note={\href{https://www.researchgate.net/profile/Mahesan-Niranjan/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems/links/5438d5db0cf204cab1d6db0f/On-Line-Q-Learning-Using-Connectionist-Systems.pdf?_sg%5B0%5D=HYd0h230b7WOR6m4hj5yx01K97aS61Z0DufUURMQr9ZqMqcEVZ0dNpG84h6uCfRl_M40FNkXgRX-GnpnxH31Ww.jBF3fgrlhaJYs3bDEaHQU22nRpKP0zKeF_oOsqh7WddL8pfxAomPSbeANzdmLP9YPB26HbLeSaEJqhFgzIxvWQ&_sg%5B1%5D=CZtZhHTEMgSwBZrpZU_7BACd8RH04JUKiITdXRQJ6MQ9SFS27jreZmcsuNcqYYWRoxcwBE-xBMbrfl1QobmEZ65bmkmpzonq5JoLRIIUKXne.jBF3fgrlhaJYs3bDEaHQU22nRpKP0zKeF_oOsqh7WddL8pfxAomPSbeANzdmLP9YPB26HbLeSaEJqhFgzIxvWQ&_iepl=}{\textbf{Article}}},
}

@article{kaelbling1998planning,
    title={Planning and acting in partially observable stochastic domains},
    author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
    journal={Artificial intelligence},
    volume={101},
    number={1-2},
    pages={99--134},
    year={1998},
    publisher={Elsevier},
    note={\href{https://www.sciencedirect.com/science/article/pii/S000437029800023X}{\textbf{Article}}},
}

@book{sutton2018reinforcement,
    title={Reinforcement Learning: An Introduction},
    author={Sutton, Richard S. and Barto, Andrew G.},
    year={1998},
    publisher={MIT Press},
    note={\href{http://incompleteideas.net/book/RLbook2020.pdf}{\textbf{Book}}},
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999},
  note={\href{https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html}{\textbf{Article}}},
}
