\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}[
        timeline/.style={draw, thick, -{Stealth[length=3mm]}, line cap=round},
        connect/.style={thick, dashed}
    ]

    %=== Timeline scale parameters ===%
    \def\yearmin{1943}
    \def\yearmax{2025}
    \def\scalefactor{0.3} % cm per year, to fit everything

    % Main timeline
    \draw[timeline] (0,0) -- ({(\yearmax-\yearmin)*\scalefactor},0);

    %=== Event Macro ===%
    \newcommand{\event}[8]{
        \pgfmathsetmacro{\x}{(#1-\yearmin)*\scalefactor}
        \pgfmathsetmacro{\y}{#6}

        \begin{scope}[on background layer]
            \draw[connect] (\x,0) -- ++(0,\y);
        \end{scope}

        \node[
            draw,
            rounded corners,
            very thick,
            fill=#8,
            minimum height=#5,
            text width=#4,
            align=center,
            font=#7
        ] at (\x, \y) {\textbf{\cite{#2}} \\[2pt] #3};
    }

    %=== EVENTS ===%
    \event{1943}{mcculloch1943logical}{
    First mathematical model of an artificial neuron, forming the basis of neural networks.}{5.5cm}{1cm}{0.05cm}{\small}{blue!10};

    \event{1958}{rosenblatt1958perceptron}{
    Introduction of the Perceptron, the first trainable connectionist learning algorithm.}{5.5cm}{1cm}{-0.05cm}{\small}{cyan!10};

    \event{1986}{rumelhart1986learning}{
    Popularization of backpropagation, enabling the training of multi-layer neural networks.}{5.5cm}{1cm}{-0.05cm}{\small}{red!10};

    \event{1989}{lecun1989backpropagation}{
    Introduction of Convolutional Neural Networks, capable of processing image inputs.}{5.5cm}{1cm}{0.05cm}{\small}{purple!10};

    \event{1997}{hochreiter1997long}{
    Introduction of LSTMs, enabling neural networks to model long-term temporal dependencies.}{6cm}{1cm}{0.125cm}{\small}{green!10};

    \event{2009}{raina2009large}{
    Demonstration of GPU acceleration for large-scale neural network training.}{5.5cm}{1cm}{-0.05cm}{\small}{orange!10};

    \event{2016}{Goodfellow-et-al-2016}{
    Publication of the reference \textbf{Deep Learning Book}.}{5.5cm}{1cm}{0.05cm}{\small}{yellow!15};

    \event{2017}{vaswani2017attention}{
    Transformer architecture, enabling efficient processing of long and variable-length sequences.}{5.5cm}{1cm}{-0.125cm}{\small}{cyan!10};
    \end{tikzpicture}
    }
\end{figure}
