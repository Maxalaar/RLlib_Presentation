\begin{frame}[fragile]
\vfill
\centering
\footnotesize
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{>{\ttfamily}l l}
\toprule
\multicolumn{2}{c}{\textbf{Environment Configuration}} \\
\midrule
game\_name & 'SpaceInvaders-v5' \\
repeat\_action\_probability & 0.05 \\
frameskip & 5 \\
resize\_observation\_shape & (64, 64) \\
convert\_to\_grayscale & True \\
reward\_scale\_factor & 0.05 \\
frame\_stack\_len & 4 \\
normalize\_observation & True \\
observation\_numpy\_type & np.float16 \\
\midrule
\multicolumn{2}{c}{\textbf{Policy Architecture Configuration}} \\
\midrule
architecture & CnnPPO \\
configuration\_cnn & [(16,4,2),(32,4,2),(64,4,2),(128,4,2)] \\
configuration\_hidden\_layers & [512,256,128] \\
activation\_function\_class & LeakyReLU \\
use\_layer\_normalization\_cnn & True \\
use\_share\_cnn & True \\
\midrule
\multicolumn{2}{c}{\textbf{Reinforcement Learning Configuration}} \\
\midrule
algorithm\_name & 'PPO' \\
rollout\_fragment\_length & 2048 \\
train\_batch\_size & 2048 * 8 \\
minibatch\_size & 2048 \\
lambda\_gae & 0.95 \\
kullback\_leibler\_coefficient & 0.5 \\
clip\_policy\_parameter & 0.1 \\
clip\_value\_function\_parameter & 10 \\
entropy\_coefficient & 0.01 \\
number\_epochs & 10 \\
learning\_rate & 0.00015 \\
gradient\_clip & 100.0 \\
gradient\_clip\_by & 'global\_norm' \\
\bottomrule
\end{tabular}
}
\vfill
\end{frame}
